{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5449ea86-ff90-45e0-a606-acba66cf853b",
   "metadata": {},
   "source": [
    "# CNN with Embeddings - Post Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a18349-ccf4-4ded-8acb-42c6231c18f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f6a3e3-c615-4f78-80f8-806b92882c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaredfeldman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaredfeldman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for stop words\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# for standardizing text\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1e523-4819-4cb8-86b4-4a413fc8c788",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initial Data Load and File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a5dfc6-23be-4461-a6c3-2cd693f8e517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>number_of_downvotes_of_request_at_retrieval</th>\n",
       "      <th>number_of_upvotes_of_request_at_retrieval</th>\n",
       "      <th>post_was_edited</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_number_of_comments_at_retrieval</th>\n",
       "      <th>request_text</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "      <th>requester_subreddits_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_retrieval</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_retrieval</th>\n",
       "      <th>requester_user_flair</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>nickylvst</td>\n",
       "      <td>1317852607</td>\n",
       "      <td>1317849007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>0</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>501.1111</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, Eve, IAmA, MontereyBay, RandomKind...</td>\n",
       "      <td>34</td>\n",
       "      <td>4258</td>\n",
       "      <td>116</td>\n",
       "      <td>11168</td>\n",
       "      <td>None</td>\n",
       "      <td>fohacidal</td>\n",
       "      <td>1332652424</td>\n",
       "      <td>1332648824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\n",
       "0                     N/A                                            0   \n",
       "1                     N/A                                            2   \n",
       "\n",
       "   number_of_upvotes_of_request_at_retrieval  post_was_edited request_id  \\\n",
       "0                                          1                0   t3_l25d7   \n",
       "1                                          5                0   t3_rcb83   \n",
       "\n",
       "   request_number_of_comments_at_retrieval  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "                                        request_text  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0            Request Colorado Springs Help Us Please   \n",
       "1  [Request] California, No cash and I could use ...   \n",
       "\n",
       "   requester_account_age_in_days_at_request  ...  requester_received_pizza  \\\n",
       "0                                    0.0000  ...                     False   \n",
       "1                                  501.1111  ...                     False   \n",
       "\n",
       "                     requester_subreddits_at_request  \\\n",
       "0                                                 []   \n",
       "1  [AskReddit, Eve, IAmA, MontereyBay, RandomKind...   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_request  \\\n",
       "0                                             0   \n",
       "1                                            34   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_retrieval  \\\n",
       "0                                               1   \n",
       "1                                            4258   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request  \\\n",
       "0                                            0   \n",
       "1                                          116   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_retrieval  requester_user_flair  \\\n",
       "0                                              1                  None   \n",
       "1                                          11168                  None   \n",
       "\n",
       "   requester_username  unix_timestamp_of_request  \\\n",
       "0           nickylvst                 1317852607   \n",
       "1           fohacidal                 1332652424   \n",
       "\n",
       "   unix_timestamp_of_request_utc  \n",
       "0                     1317849007  \n",
       "1                     1332648824  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "training_data = pd.read_json(\"train.json\")\n",
    "training_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b33c95-7cda-4de7-a165-f47ac5595a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "request_text_edit_aware     object\n",
       "requester_received_pizza      bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_text = training_data[['request_text_edit_aware', 'requester_received_pizza']]\n",
    "training_data_text.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8479de64-22a3-43af-956b-407afebb1414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>Is anyone out there kind enough to help me out...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>If someone could hook me up with a $15 gift ca...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>Have today off, soo I'll be stuck in the house...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>I've never done anything like this before, but...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>Like the title says, had to pay an unexpected ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4040 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                request_text_edit_aware  \\\n",
       "0     Hi I am in need of food for my 4 children we a...   \n",
       "1     I spent the last money I had on gas today. Im ...   \n",
       "2     My girlfriend decided it would be a good idea ...   \n",
       "3     It's cold, I'n hungry, and to be completely ho...   \n",
       "4     hey guys:\\n I love this sub. I think it's grea...   \n",
       "...                                                 ...   \n",
       "4035  Is anyone out there kind enough to help me out...   \n",
       "4036  If someone could hook me up with a $15 gift ca...   \n",
       "4037  Have today off, soo I'll be stuck in the house...   \n",
       "4038  I've never done anything like this before, but...   \n",
       "4039  Like the title says, had to pay an unexpected ...   \n",
       "\n",
       "      requester_received_pizza  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                        False  \n",
       "3                        False  \n",
       "4                        False  \n",
       "...                        ...  \n",
       "4035                     False  \n",
       "4036                      True  \n",
       "4037                     False  \n",
       "4038                     False  \n",
       "4039                     False  \n",
       "\n",
       "[4040 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70db21df-65ff-458e-abb2-482291cf9075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "3046\n",
      "4040\n"
     ]
    }
   ],
   "source": [
    "posts_pizza = training_data_text[training_data_text['requester_received_pizza'] == True]\n",
    "posts_no_pizza = training_data_text[training_data_text['requester_received_pizza'] == False]\n",
    "\n",
    "print(len(posts_pizza))\n",
    "print(len(posts_no_pizza))\n",
    "print(len(posts_pizza) + len(posts_no_pizza))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdfacc8-5b46-4725-aff7-ad67778aef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "3046\n",
      "4040\n"
     ]
    }
   ],
   "source": [
    "posts_pizza_list = list(posts_pizza['request_text_edit_aware'])\n",
    "posts_no_pizza_list = list(posts_no_pizza['request_text_edit_aware'])\n",
    "\n",
    "print(len(posts_pizza_list))\n",
    "print(len(posts_no_pizza_list))\n",
    "print(len(posts_pizza_list) + len(posts_no_pizza_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cc1497-fdbe-47fc-8153-3aa48d1e3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# create files for each list\n",
    "# uncomment below to run but files already created and moved\n",
    "\n",
    "#for index, post in enumerate(posts_no_pizza_list):\n",
    "    #print(index)\n",
    "    #print(post)\n",
    "    #with io.open(\"file_\" + str(index) + \".txt\", 'w', encoding = 'utf-8') as f:\n",
    "        #f.write(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2298ad-3aab-4016-a714-cac11bcb81f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "468135d6-dfee-47de-879b-9f8aefa7e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    # Convert the input_data to lowercase\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    \n",
    "    # Remove any '<br />' tags from the text and replace them with a space\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    \n",
    "    # Remove any punctuation from the text\n",
    "    # 're.escape(string.punctuation)' escapes all punctuation characters for use in the regex pattern\n",
    "    # The regular expression pattern '[%s]' % re.escape(string.punctuation) matches any punctuation character\n",
    "    # and replaces it with an empty string, effectively removing it from the text.\n",
    "    # For example, if the input_data is \"Hello, world!\", the regex_replace will return \"Hello world\"    \n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f704ac2-07c9-4d48-8b1e-d0551381727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a learning rate schedule to reduce learning rate each epoch\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.01 # initial learning rate\n",
    "    decay_steps = 5 # number of epochs to start decay\n",
    "    decay_rate = 0.5 # decay rate\n",
    "\n",
    "    # Compute the learning rate for the current epoch using exponential decay\n",
    "    # The formula used is: lr = initial_learning_rate * (decay_rate ** (epoch // decay_steps))\n",
    "    # The double division '//' ensures integer division so that only after 'decay_steps' epochs,\n",
    "    # the learning rate gets reduced.\n",
    "    lr = initial_learning_rate * (decay_rate ** (epoch // decay_steps))\n",
    "    \n",
    "    # Return the computed learning rate for the current epoch    \n",
    "    return lr\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "# The LearningRateScheduler callback will call the 'lr_schedule' function\n",
    "# at the beginning of each epoch to determine the learning rate for that epoch.\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181835d6-dc9f-4723-b8c6-15f4957f02f1",
   "metadata": {},
   "source": [
    "## Model 1: All post content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78bd86a-4172-4a02-a538-e5cf56401f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4040 files belonging to 2 classes.\n",
      "Using 3636 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # divide dataset into batches of 32 samples each\n",
    "seed = 123 # for reproduceability\n",
    "\n",
    "# raw_train_ds will be a TensorFlow dataset that contains\n",
    "# batches of text data and their corresponding labels\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'posts/all_posts', # pull data from this directory\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c8c5ee1-c361-4700-954c-aae7826213d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post b\"Broke ass-college student here; got paid 10 days ago and all of my paycheck went to rent and bills. I've got no groceries in the house and all my roommates are back home for the summer. No gas in the car and the bike's out of commission :(. If any kind redditor could hook me up I'd be able to eat for two days :D\\n\\ngoogle maps link to domino's in my area\\nhttp://maps.google.com/maps/place?cid=10239146004443176581&amp;q=domino's+pizza,+chico,+ca&amp;hl=en&amp;sll=39.722827,-121.848776&amp;sspn=0.012468,0.024856&amp;ie=UTF8&amp;ll=39.733066,-121.864429&amp;spn=0,0&amp;z=15\\n\\n\\nEDIT: oh, and of course: I'll be pizzaing it forward when my paycheck comes next month. this is a ridiculously cool subreddit.\"\n",
      "Label 1\n",
      "Post b\"21/m/sfbay multi year lurker first time poster!\\nme! -&gt; http://imgur.com/VXtY6\\n\\nI'm just really hungry and I've got no money right now. Been unemployed for 6 months but just got a real great job offer at a consulting firm in berkeley. so i'd like to do something special to celebrate but i'm in the red right now.\\n\\nafter jan 2nd i will be employed with a well paying job and will definitely pay it forward.\\n\\nI'm stoked!\"\n",
      "Label 1\n",
      "Post b'would love a pizza. pm me if you wanna be kind :D'\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "# print some examples\n",
    "\n",
    "# Take one batch (batch_size=32) from the 'raw_train_ds' dataset\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    # Loop over the first three samples in the batch\n",
    "    for i in range(3):\n",
    "        # Convert and print the text data and their corresponding labels for each sample\n",
    "        print(\"Post\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cbdd449-311a-4eef-bc62-9ee9c0e7e886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to no_pizza\n",
      "Label 1 corresponds to pizza\n"
     ]
    }
   ],
   "source": [
    "# print what each label (0 or 1) corresponds to\n",
    "\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a3f6da-c65a-4887-86d8-6fb2e665f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4040 files belonging to 2 classes.\n",
      "Using 404 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow dataset for validation from text files stored in the directory\n",
    "# The 'text_dataset_from_directory' function automatically labels the text data based on subdirectories.\n",
    "# In this case, text files are stored in the 'posts/all_posts' directory, and subdirectories inside 'all_posts'\n",
    "# represent different classes or categories of text data.\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'posts/all_posts', # directory where .txt files are stored\n",
    "    batch_size=batch_size, # same as above\n",
    "    validation_split=0.1, # same as above\n",
    "    subset='validation', # this time, we're creating the validation set\n",
    "    seed=seed) # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b660bdfe-fee4-4ad2-a4b6-aa0b7529999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of unique tokens (words) to keep in the vocabulary\n",
    "max_features = 15000\n",
    "\n",
    "# Set the maximum sequence length of the tokenized text data\n",
    "sequence_length = 300\n",
    "\n",
    "# Create a TextVectorization layer for tokenizing and vectorizing text data\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization, # Preprocessing function for standardizing text data, defined earlier\n",
    "    max_tokens=max_features,# Maximum number of unique tokens to keep in the vocabulary\n",
    "    output_mode='int', # Output mode as integer indices (integers represent tokens)\n",
    "    output_sequence_length=sequence_length) # Maximum sequence length of the tokenized text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17c9ed89-84d4-458a-9711-0e8dbca7a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    # Expand the dimensions of the 'text' tensor to make it compatible with the 'vectorize_layer'\n",
    "    # The '-1' argument adds a new axis at the end, effectively converting the 1D tensor 'text' into a 2D tensor\n",
    "    # For example, if 'text' was [word1, word2, word3], it will become [[word1], [word2], [word3]]\n",
    "    text = tf.expand_dims(text, -1)\n",
    "\n",
    "    # Pass the expanded 'text' tensor through the 'vectorize_layer' to convert it into numerical sequences\n",
    "    # The 'vectorize_layer' was defined earlier and tokenizes the text data into integer sequences.\n",
    "    # It also applies the 'custom_standardization' function for preprocessing the text.\n",
    "    vectorized_text = vectorize_layer(text)\n",
    "\n",
    "    # Return the vectorized_text and its corresponding 'label'.\n",
    "    # 'label' is associated with the 'text' and represents the class/category of the text.\n",
    "    return vectorized_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc6434b-9c2a-4bdc-97b3-b821b238cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset 'train_text' that contains only the text data (x) from 'raw_train_ds'\n",
    "# The 'train_text' dataset is created using the 'map' function,\n",
    "# which extracts only the 'x' (text) part of the input tuple (x, y).\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Adapt the 'vectorize_layer' to the training data\n",
    "# This step is necessary to build the vocabulary and tokenize the text data based on the training dataset.\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4c64520-95ac-419e-bb58-0c081eed2731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post---> tf.Tensor(b\"I'm helping my friend out who got kicked out of his house, letting him crash here and offering my time and whatever else he needs. As a result of helping him drink his way out of this (not healthy, I know, but we all deal in different ways, and he IS looking for a new place) we're broke and I'm out of ideas to cheer him up. Then I saw this place. It would be a perfect story, and cheer him up immensely, if some random stranger on the internet helped us out. \", shape=(), dtype=string)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Label---> pizza\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vectorized post---> (<tf.Tensor: shape=(1, 300), dtype=int64, numpy=\n",
      "array([[  17,  430,    7,  191,   24,  121,   77,  949,   24,    8,  140,\n",
      "         139, 1326,  151, 2520,   62,    4, 1376,    7,   69,    4,  401,\n",
      "         245,   84,  624,   38,    5, 1852,    8,  430,  151, 1278,  140,\n",
      "         156,   24,    8,   20,   35, 1771,    2,   85,   18,   27,   40,\n",
      "         643,   10,  641, 1820,    4,   84,   16,  195,    9,    5,  106,\n",
      "         237,  100,   87,    4,   17,   24,    8, 1533,    3,  538,  151,\n",
      "          47,  165,    2,  878,   20,  237,   13,   15,   19,    5, 1442,\n",
      "         185,    4,  538,  151,   47, 2461,   26,   33,  268,  962,   21,\n",
      "           6,  534,  718,   81,   24,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# Get the next batch of data from 'raw_train_ds'\n",
    "post_batch, label_batch = next(iter(raw_train_ds))\n",
    "\n",
    "# Extract the first post (text data) and its corresponding label from the batch\n",
    "first_post, first_label = post_batch[0], label_batch[0]\n",
    "\n",
    "# Print the raw text post\n",
    "print(\"Post--->\", first_post)\n",
    "\n",
    "# Print break\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Print the label of corresponding post\n",
    "print(\"Label--->\", raw_train_ds.class_names[first_label])\n",
    "\n",
    "# Print break\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Print the vectorized post and corresponding label\n",
    "print(\"Vectorized post--->\", vectorize_text(first_post, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2a7d059-844f-4d3a-ad28-fcbe4063517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12570\n",
      "958 --->  training\n",
      "135 --->  them\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the entire vocabulary\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n",
    "\n",
    "# Print two samples, from index 958 and index 135\n",
    "print(\"958 ---> \",vectorize_layer.get_vocabulary()[958])\n",
    "print(\"135 ---> \",vectorize_layer.get_vocabulary()[135])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b41acf5b-2c56-466e-8a8d-cd9b759cf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the 'vectorize_text' function to 'raw_train_ds' to convert its text data into vectorized numerical sequences\n",
    "# The 'vectorize_text' function preprocesses and tokenizes the text data using the 'vectorize_layer'\n",
    "# The resulting dataset 'train_ds' contains batches of vectorized text data and their corresponding labels.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "\n",
    "# Map the 'vectorize_text' function to 'raw_val_ds' to convert its text data into vectorized numerical sequences\n",
    "# The 'vectorize_text' function preprocesses and tokenizes the text data using the 'vectorize_layer'\n",
    "# The resulting dataset 'val_ds' contains batches of vectorized text data and their corresponding labels.\n",
    "val_ds = raw_val_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "603c98d0-8bac-4600-9aa7-711c64316c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AUTOTUNE, which allows TensorFlow to automatically tune the buffer size for optimal performance.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Cache the 'train_ds' dataset to improve data loading speed during training.\n",
    "# Caching stores the data in memory after the first iteration through the dataset,\n",
    "# so subsequent iterations can access it faster without re-reading from the disk.\n",
    "train_ds = train_ds.cache()\n",
    "\n",
    "# Prefetch the 'train_ds' dataset to overlap data preprocessing and model execution.\n",
    "# Prefetching allows the data pipeline to fetch data for the next batch while the current batch is being processed,\n",
    "# reducing the idle time and maximizing GPU utilization during training.\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Cache the 'val_ds' dataset to improve data loading speed during validation.\n",
    "# Caching stores the data in memory after the first iteration through the dataset,\n",
    "# so subsequent iterations can access it faster without re-reading from the disk.\n",
    "val_ds = val_ds.cache()\n",
    "\n",
    "# Prefetch the 'val_ds' dataset to overlap data preprocessing and model execution during validation.\n",
    "# Prefetching allows the data pipeline to fetch data for the next batch while the current batch is being processed,\n",
    "# reducing the idle time and maximizing GPU utilization during validation.\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3de048d5-d9d6-4c65-ab74-408c57bdf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Clear previous session and set a random seed for reproducibility\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add an Embedding layer to convert text data into dense numerical vectors\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim = max_features,  # size of feature vocabulary (number of unique words), defined earlier\n",
    "        output_dim = 4,  # dimension of the embedding vector for each word (embedding dimension)\n",
    "        input_length=sequence_length  # Length of each input sequence (number of words in a post), defined earlier\n",
    "    ))\n",
    "    \n",
    "    # Add a 1D Convolutional layer to capture local patterns in the text\n",
    "    model.add(tf.keras.layers.Conv1D(\n",
    "        filters=16,        # Number of filters (output channels)\n",
    "        strides=3,         # Stride size for the convolution operation\n",
    "        padding='same',    # Padding to ensure the output has the same length as the input\n",
    "        kernel_size=12,    # Length of the convolutional kernel (window size)\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    # Add a Global Average Pooling layer to aggregate information over the sequence dimension\n",
    "    # This reduces the sequence length to 1, making each post represented by a single vector\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    \n",
    "    # Alternatively, we could concatenate the embedding representations of \n",
    "    # all tokens in the movie review\n",
    "    #model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # Add an additional Dense layer with 16 units and ReLU activation\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=16,        \n",
    "      activation='relu'))\n",
    "    \n",
    "    # Add the output layer with 1 unit and sigmoid activation for binary classification    \n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=1,        \n",
    "      activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with binary cross-entropy loss for binary classification\n",
    "    # Use the Adam optimizer with default learning rate\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),#learning_rate = 0.001), commented out to use learning rate schedule\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d14596-a952-4ece-b28f-62ff90b0aca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.embedding.Embedding at 0x282db6a30>,\n",
       " <keras.src.layers.convolutional.conv1d.Conv1D at 0x28808ba90>,\n",
       " <keras.src.layers.pooling.global_average_pooling1d.GlobalAveragePooling1D at 0x28815fb20>,\n",
       " <keras.src.layers.core.dense.Dense at 0x288a23640>,\n",
       " <keras.src.layers.core.dense.Dense at 0x288a237c0>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 4)            60000     \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100, 16)           784       \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61073 (238.57 KB)\n",
      "Trainable params: 61073 (238.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the model using the build_model function\n",
    "model_all_posts = build_model()\n",
    "\n",
    "# Display the model layers.\n",
    "display(model_all_posts.layers)\n",
    "\n",
    "# Display a summary of the mdel architecture\n",
    "display(model_all_posts.summary())\n",
    "\n",
    "# Retrieve the embeddings layer weights\n",
    "# The embeddings are stored in the last layer of the model\n",
    "embeddings = model_all_posts.layers[-1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dcda660-194a-4c88-936d-a5d96a2e1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "114/114 [==============================] - 4s 36ms/step - loss: 0.5992 - accuracy: 0.7349 - val_loss: 0.5516 - val_accuracy: 0.7574\n",
      "Epoch 2/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.5506 - accuracy: 0.7536 - val_loss: 0.5515 - val_accuracy: 0.7574\n",
      "Epoch 3/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.5457 - accuracy: 0.7544 - val_loss: 0.5506 - val_accuracy: 0.7574\n",
      "Epoch 4/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.5266 - accuracy: 0.7607 - val_loss: 0.5535 - val_accuracy: 0.7401\n",
      "Epoch 5/15\n",
      "114/114 [==============================] - 3s 26ms/step - loss: 0.4884 - accuracy: 0.7855 - val_loss: 0.5635 - val_accuracy: 0.7203\n",
      "Epoch 6/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.4380 - accuracy: 0.8240 - val_loss: 0.5765 - val_accuracy: 0.7030\n",
      "Epoch 7/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.3834 - accuracy: 0.8573 - val_loss: 0.5859 - val_accuracy: 0.7203\n",
      "Epoch 8/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.3285 - accuracy: 0.8839 - val_loss: 0.6002 - val_accuracy: 0.7302\n",
      "Epoch 9/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.2741 - accuracy: 0.9076 - val_loss: 0.6149 - val_accuracy: 0.7277\n",
      "Epoch 10/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.2213 - accuracy: 0.9318 - val_loss: 0.6357 - val_accuracy: 0.7153\n",
      "Epoch 11/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.1787 - accuracy: 0.9491 - val_loss: 0.6614 - val_accuracy: 0.7104\n",
      "Epoch 12/15\n",
      "114/114 [==============================] - 3s 24ms/step - loss: 0.1458 - accuracy: 0.9587 - val_loss: 0.6944 - val_accuracy: 0.6906\n",
      "Epoch 13/15\n",
      "114/114 [==============================] - 3s 25ms/step - loss: 0.1208 - accuracy: 0.9689 - val_loss: 0.7274 - val_accuracy: 0.7030\n",
      "Epoch 14/15\n",
      "114/114 [==============================] - 3s 24ms/step - loss: 0.1017 - accuracy: 0.9761 - val_loss: 0.7665 - val_accuracy: 0.7104\n",
      "Epoch 15/15\n",
      "114/114 [==============================] - 3s 24ms/step - loss: 0.0872 - accuracy: 0.9791 - val_loss: 0.8189 - val_accuracy: 0.7302\n"
     ]
    }
   ],
   "source": [
    "history = model_all_posts.fit(\n",
    "    train_ds,\n",
    "    batch_size = 16,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    validation_split=0.1)#,\n",
    "    #callbacks=[lr_scheduler]) # with this commented out, we'll use the default learning rate of 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e452d-82a2-4211-acde-6bfaf091085c",
   "metadata": {},
   "source": [
    "## Model 2: 700 Samples of Each Class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
